{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Install app requirements",
   "id": "cb748af2228b7fec"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -r requirements.txt\n",
    "!source .venv/bin/activate"
   ],
   "id": "1477e7c4849d854f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "___",
   "id": "db658ab34da62a92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load environment variables",
   "id": "c99c4241b7c986d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:52:17.152926Z",
     "start_time": "2025-08-26T09:52:17.149201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from commons.constants import *\n",
    "mdb_url = MONGO_PRODUCTS_COLLECTION_URL"
   ],
   "id": "33af75b69bb6ed68",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Connect to Spark Cluster using SparkConnect",
   "id": "22672601f232d3c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:52:21.210001Z",
     "start_time": "2025-08-26T09:52:20.853952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from analytics.sc_session import get_session\n",
    "session = get_session()\n",
    "session"
   ],
   "id": "51b235d562d60ca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.session.SparkSession at 0x106ed9870>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:52:25.574415Z",
     "start_time": "2025-08-26T09:52:25.506993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from analytics.sc_data import load_products_collection\n",
    "df = load_products_collection(session)\n",
    "df"
   ],
   "id": "58cbf9b0965bdc94",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_id: struct<product_id:bigint>, product_created_at: string, product_deleted_at: void, product_name: string, product_properties: string, product_sku: string, product_updated_at: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.printSchema()",
   "id": "d7ec4f71e262bf32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.createOrReplaceTempView(\"mbdtfm_magento_products_tmp\")\n",
    "df5 = session.sql(\"\"\"\n",
    "    select _id.product_id, product_name\n",
    "    from mbdtfm_magento_products_tmp\n",
    "    where product_deleted_at is null\n",
    "    order by product_name asc limit 5\n",
    "  \"\"\")\n",
    "df5.show()"
   ],
   "id": "e62a9322e387ab1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:52:39.952260Z",
     "start_time": "2025-08-26T09:52:38.300013Z"
    }
   },
   "cell_type": "code",
   "source": "df.show(n=10)",
   "id": "a973741c92b1dd01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+--------------------+--------------------+-----------+--------------------+\n",
      "| _id|  product_created_at|product_deleted_at|        product_name|  product_properties|product_sku|  product_updated_at|\n",
      "+----+--------------------+------------------+--------------------+--------------------+-----------+--------------------+\n",
      "| {2}|2025-08-08T09:29:58Z|              NULL|Strive Shoulder Pack|{\"product_sku\":\"2...|    24-MB04|2025-08-08T09:29:58Z|\n",
      "| {3}|2025-08-08T09:29:58Z|              NULL|Crown Summit Back...|{\"product_sku\": \"...|    24-MB03|2025-08-08T09:29:58Z|\n",
      "| {5}|2025-08-08T09:29:58Z|              NULL|Rival Field Messe...|{\"product_sku\":\"2...|    24-MB06|2025-08-08T09:29:58Z|\n",
      "| {6}|2025-08-08T09:29:58Z|              NULL|     Fusion Backpack|{\"product_sku\": \"...|    24-MB02|2025-08-08T09:29:58Z|\n",
      "| {8}|2025-08-08T09:29:59Z|              NULL|     Voyage Yoga Bag|{\"product_sku\": \"...|    24-WB01|2025-08-08T09:29:59Z|\n",
      "|{10}|2025-08-08T09:29:59Z|              NULL| Savvy Shoulder Tote|{\"product_sku\":\"2...|    24-WB05|2025-08-08T09:29:59Z|\n",
      "|{12}|2025-08-08T09:29:59Z|              NULL|     Driven Backpack|{\"product_sku\": \"...|    24-WB03|2025-08-08T09:29:59Z|\n",
      "|{13}|2025-08-08T09:29:59Z|              NULL|    Overnight Duffle|{\"product_sku\":\"2...|    24-WB07|2025-08-08T09:29:59Z|\n",
      "|{15}|2025-08-08T09:29:59Z|              NULL|Affirm Water Bottle |{\"product_sku\": \"...|    24-UG06|2025-08-08T09:29:59Z|\n",
      "|{21}|2025-08-08T09:29:59Z|              NULL|Sprite Foam Yoga ...|{\"product_sku\": \"...|   24-WG084|2025-08-08T09:29:59Z|\n",
      "+----+--------------------+------------------+--------------------+--------------------+-----------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe().show()",
   "id": "167df089308287ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.columns",
   "id": "df92ef563fb1d972",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:52:49.465072Z",
     "start_time": "2025-08-26T09:52:49.404760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "from typing import Dict\n",
    "\n",
    "def create_product_embeddings(df: DataFrame, properties_column: str = \"product_properties\", embeddings_column: str = \"product_embeddings\") -> None:\n",
    "    \"\"\"\n",
    "    Reads products from MongoDB, calculates embeddings for the `product_properties`\n",
    "    field in collection `mbdtfm_magento_catalog_products_mview`, and writes back to MongoDB\n",
    "    with the new `product_embeddings` column in the same collection.\n",
    "    :param df: DataFrame containing the `product_properties` column to create embeddings from.\n",
    "    :param properties_column: Name of the `product_properties` column to create embeddings from.\n",
    "    :param embeddings_column: Name of the `product_embeddings` column to create embeddings to.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    flatten_udf = udf(__flatten_json, StringType())\n",
    "    df_flat = df.withColumn(f\"{properties_column}_flat\", flatten_udf(col(properties_column)))\n",
    "    source_column = f\"{properties_column}_flat\"\n",
    "    df = __compute_embeddings(df_flat, embeddings_column, source_column)\n",
    "    (df.write.format(\"mongodb\")\n",
    "        .option(\"spark.mongodb.write.connection.uri\", MONGO_URL)\n",
    "        .option(\"database\", MONGO_DATABASE)\n",
    "        .option(\"collection\", MONGO_PRODUCTS_COLLECTION)\n",
    "        .mode(\"append\")\n",
    "        .save())\n",
    "\n",
    "\n",
    "def __compute_embeddings(df: DataFrame, target_column: str, source_column: str) -> DataFrame:\n",
    "    tokenizer = Tokenizer(inputCol=source_column, outputCol=\"tokens\")\n",
    "    tokenized = tokenizer.transform(df)\n",
    "    hashing_tf = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\", numFeatures=128)\n",
    "    featurized = hashing_tf.transform(tokenized)\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=target_column)\n",
    "    idf_model = idf.fit(featurized)\n",
    "    return idf_model.transform(featurized)\n",
    "\n",
    "def __flatten_json(json_str: str) -> str:\n",
    "    try:\n",
    "        flat = __flatten(json.loads(json_str))\n",
    "        return ' '.join([f\"{k}={v}\" for k, v in flat.items()])\n",
    "    except Exception:\n",
    "        return ''\n",
    "\n",
    "def __flatten(obj: Dict, prefix: str = '') -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Recursive helper to flatten JSON objects inside a UDF, preparing data for tokenization.\n",
    "    :param obj: JSON object.\n",
    "    :param prefix: prefix to prepend to all keys.\n",
    "    :return: flattened dictionary.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for k, v in obj.items():\n",
    "        new_key = f\"{prefix}{k}\" if prefix == '' else f\"{prefix}.{k}\"\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(__flatten(v, new_key).items()) # Recursive call.\n",
    "        elif isinstance(v, list):\n",
    "            val_str = ' '.join([str(item) for item in v])\n",
    "            items.append((new_key, val_str))\n",
    "        else:\n",
    "            items.append((new_key, str(v)))\n",
    "    return dict(items)"
   ],
   "id": "74e72ba38d850e0b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:55:44.499361Z",
     "start_time": "2025-08-26T09:52:56.871634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "create_product_embeddings(df)\n",
    "\n",
    "df.columns"
   ],
   "id": "e03d08f8edd45b59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------------+--------------------+--------------------+-----------+--------------------+-----------------------+\n",
      "|_id|  product_created_at|product_deleted_at|        product_name|  product_properties|product_sku|  product_updated_at|product_properties_flat|\n",
      "+---+--------------------+------------------+--------------------+--------------------+-----------+--------------------+-----------------------+\n",
      "|{2}|2025-08-08T09:29:58Z|              NULL|Strive Shoulder Pack|{\"product_sku\":\"2...|    24-MB04|2025-08-08T09:29:58Z|   product_sku=24-MB...|\n",
      "|{3}|2025-08-08T09:29:58Z|              NULL|Crown Summit Back...|{\"product_sku\": \"...|    24-MB03|2025-08-08T09:29:58Z|   product_sku=24-MB...|\n",
      "|{5}|2025-08-08T09:29:58Z|              NULL|Rival Field Messe...|{\"product_sku\":\"2...|    24-MB06|2025-08-08T09:29:58Z|   product_sku=24-MB...|\n",
      "|{6}|2025-08-08T09:29:58Z|              NULL|     Fusion Backpack|{\"product_sku\": \"...|    24-MB02|2025-08-08T09:29:58Z|   product_sku=24-MB...|\n",
      "|{8}|2025-08-08T09:29:59Z|              NULL|     Voyage Yoga Bag|{\"product_sku\": \"...|    24-WB01|2025-08-08T09:29:59Z|   product_sku=24-WB...|\n",
      "+---+--------------------+------------------+--------------------+--------------------+-----------+--------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "+---+--------------------+------------------+--------------------+--------------------+-----------+--------------------+-----------------------+--------------------+--------------------+--------------------+\n",
      "|_id|  product_created_at|product_deleted_at|        product_name|  product_properties|product_sku|  product_updated_at|product_properties_flat|              tokens|         rawFeatures|  product_embeddings|\n",
      "+---+--------------------+------------------+--------------------+--------------------+-----------+--------------------+-----------------------+--------------------+--------------------+--------------------+\n",
      "|{2}|2025-08-08T09:29:58Z|              NULL|Strive Shoulder Pack|{\"product_sku\":\"2...|    24-MB04|2025-08-08T09:29:58Z|   product_sku=24-MB...|[product_sku=24-m...|(128,[0,1,2,3,4,5...|(128,[0,1,2,3,4,5...|\n",
      "|{3}|2025-08-08T09:29:58Z|              NULL|Crown Summit Back...|{\"product_sku\": \"...|    24-MB03|2025-08-08T09:29:58Z|   product_sku=24-MB...|[product_sku=24-m...|(128,[0,1,2,4,5,6...|(128,[0,1,2,4,5,6...|\n",
      "|{5}|2025-08-08T09:29:58Z|              NULL|Rival Field Messe...|{\"product_sku\":\"2...|    24-MB06|2025-08-08T09:29:58Z|   product_sku=24-MB...|[product_sku=24-m...|(128,[0,1,2,3,4,5...|(128,[0,1,2,3,4,5...|\n",
      "|{6}|2025-08-08T09:29:58Z|              NULL|     Fusion Backpack|{\"product_sku\": \"...|    24-MB02|2025-08-08T09:29:58Z|   product_sku=24-MB...|[product_sku=24-m...|(128,[0,1,2,4,5,6...|(128,[0,1,2,4,5,6...|\n",
      "|{8}|2025-08-08T09:29:59Z|              NULL|     Voyage Yoga Bag|{\"product_sku\": \"...|    24-WB01|2025-08-08T09:29:59Z|   product_sku=24-WB...|[product_sku=24-w...|(128,[0,1,2,4,5,6...|(128,[0,1,2,4,5,6...|\n",
      "+---+--------------------+------------------+--------------------+--------------------+-----------+--------------------+-----------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "ename": "SparkException",
     "evalue": "Writing job failed.\n\nJVM stacktrace:\norg.apache.spark.SparkException\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobFailedError(QueryExecutionErrors.scala:893)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:452)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:397)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:237)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:358)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:237)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:190)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:2954)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2492)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 7) (172.19.0.4 executor 6): com.mongodb.spark.sql.connector.exceptions.DataException: Cannot cast (128,[0,1,2,3,4,5,6,7,8,10,11,12,13,14,15,16,17,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,38,39,40,41,42,43,44,45,46,47,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,98,99,100,102,103,104,105,106,107,108,109,111,112,113,114,115,116,117,118,119,120,121,122,123,124,126,127],[3.0,2.0,5.0,2.0,7.0,5.0,1.0,1.0,5.0,2.0,3.0,1.0,3.0,1.0,2.0,1.0,8.0,4.0,1.0,1.0,2.0,3.0,1.0,2.0,3.0,1.0,4.0,4.0,6.0,2.0,3.0,1.0,1.0,3.0,2.0,2.0,4.0,2.0,5.0,5.0,3.0,5.0,2.0,1.0,3.0,3.0,1.0,1.0,5.0,2.0,2.0,4.0,3.0,3.0,5.0,4.0,3.0,5.0,3.0,4.0,3.0,3.0,3.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,4.0,2.0,3.0,6.0,2.0,4.0,3.0,2.0,1.0,3.0,2.0,3.0,8.0,3.0,2.0,2.0,3.0,1.0,5.0,1.0,4.0,3.0,5.0,3.0,2.0,2.0,2.0,3.0,2.0,3.0,5.0,5.0,2.0,1.0,5.0,1.0,2.0,1.0,3.0,2.0,5.0,5.0,2.0,1.0,1.0,2.0,3.0,6.0]) into a BsonValue. org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 has no matching BsonValue. Error: Cannot cast (128,[0,1,2,3,4,5,6,7,8,10,11,12,13,14,15,16,17,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,38,39,40,41,42,43,44,45,46,47,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,98,99,100,102,103,104,105,106,107,108,109,111,112,113,114,115,116,117,118,119,120,121,122,123,124,126,127],[3.0,2.0,5.0,2.0,7.0,5.0,1.0,1.0,5.0,2.0,3.0,1.0,3.0,1.0,2.0,1.0,8.0,4.0,1.0,1.0,2.0,3.0,1.0,2.0,3.0,1.0,4.0,4.0,6.0,2.0,3.0,1.0,1.0,3.0,2.0,2.0,4.0,2.0,5.0,5.0,3.0,5.0,2.0,1.0,3.0,3.0,1.0,1.0,5.0,2.0,2.0,4.0,3.0,3.0,5.0,4.0,3.0,5.0,3.0,4.0,3.0,3.0,3.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,4.0,2.0,3.0,6.0,2.0,4.0,3.0,2.0,1.0,3.0,2.0,3.0,8.0,3.0,2.0,2.0,3.0,1.0,5.0,1.0,4.0,3.0,5.0,3.0,2.0,2.0,2.0,3.0,2.0,3.0,5.0,5.0,2.0,1.0,5.0,1.0,2.0,1.0,3.0,2.0,5.0,5.0,2.0,1.0,1.0,2.0,3.0,6.0]) into a BsonValue. org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 data type has no matching BsonValue.\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.lambda$createObjectToBsonValue$be3154de$1(RowToBsonDocumentConverter.java:140)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.lambda$dataTypeToBsonElement$300467b5$1(RowToBsonDocumentConverter.java:153)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.rowToBsonDocument(RowToBsonDocumentConverter.java:270)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.fromRow(RowToBsonDocumentConverter.java:116)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.fromRow(RowToBsonDocumentConverter.java:105)\n\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:91)\n\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:44)\n\tat org.apache.spark.sql.connector.write.DataWriter.writeAll(DataWriter.java:108)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:587)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:483)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:535)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:466)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:584)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:427)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: com.mongodb.spark.sql.connector.exceptions.DataException: Write aborted for: PartitionId: 0, TaskId: 7. Manual data clean up may be required.\n\t\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.abort(MongoDataWriter.java:121)\n\t\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$10(WriteToDataSourceV2Exec.scala:528)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1334)\n\t\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:424)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:397)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:237)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:358)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:237)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:190)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:2954)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2492)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)\nCaused by: com.mongodb.spark.sql.connector.exceptions.DataException: Cannot cast (128,[0,1,2,3,4,5,6,7,8,10,11,12,13,14,15,16,17,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,38,39,40,41,42,43,44,45,46,47,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,98,99,100,102,103,104,105,106,107,108,109,111,112,113,114,115,116,117,118,119,120,121,122,123,124,126,127],[3.0,2.0,5.0,2.0,7.0,5.0,1.0,1.0,5.0,2.0,3.0,1.0,3.0,1.0,2.0,1.0,8.0,4.0,1.0,1.0,2.0,3.0,1.0,2.0,3.0,1.0,4.0,4.0,6.0,2.0,3.0,1.0,1.0,3.0,2.0,2.0,4.0,2.0,5.0,5.0,3.0,5.0,2.0,1.0,3.0,3.0,1.0,1.0,5.0,2.0,2.0,4.0,3.0,3.0,5.0,4.0,3.0,5.0,3.0,4.0,3.0,3.0,3.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,4.0,2.0,3.0,6.0,2.0,4.0,3.0,2.0,1.0,3.0,2.0,3.0,8.0,3.0,2.0,2.0,3.0,1.0,5.0,1.0,4.0,3.0,5.0,3.0,2.0,2.0,2.0,3.0,2.0,3.0,5.0,5.0,2.0,1.0,5.0,1.0,2.0,1.0,3.0,2.0,5.0,5.0,2.0,1.0,1.0,2.0,3.0,6.0]) into a BsonValue. org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 has no matching BsonValue. Error: Cannot cast (128,[0,1,2,3,4,5,6,7,8,10,11,12,13,14,15,16,17,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,38,39,40,41,42,43,44,45,46,47,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,98,99,100,102,103,104,105,106,107,108,109,111,112,113,114,115,116,117,118,119,120,121,122,123,124,126,127],[3.0,2.0,5.0,2.0,7.0,5.0,1.0,1.0,5.0,2.0,3.0,1.0,3.0,1.0,2.0,1.0,8.0,4.0,1.0,1.0,2.0,3.0,1.0,2.0,3.0,1.0,4.0,4.0,6.0,2.0,3.0,1.0,1.0,3.0,2.0,2.0,4.0,2.0,5.0,5.0,3.0,5.0,2.0,1.0,3.0,3.0,1.0,1.0,5.0,2.0,2.0,4.0,3.0,3.0,5.0,4.0,3.0,5.0,3.0,4.0,3.0,3.0,3.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,4.0,2.0,3.0,6.0,2.0,4.0,3.0,2.0,1.0,3.0,2.0,3.0,8.0,3.0,2.0,2.0,3.0,1.0,5.0,1.0,4.0,3.0,5.0,3.0,2.0,2.0,2.0,3.0,2.0,3.0,5.0,5.0,2.0,1.0,5.0,1.0,2.0,1.0,3.0,2.0,5.0,5.0,2.0,1.0,1.0,2.0,3.0,6.0]) into a BsonValue. org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 data type has no matching BsonValue.\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.lambda$createObjectToBsonValue$be3154de$1(RowToBsonDocumentConverter.java:140)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.lambda$dataTypeToBsonElement$300467b5$1(RowToBsonDocumentConverter.java:153)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.rowToBsonDocument(RowToBsonDocumentConverter.java:270)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.fromRow(RowToBsonDocumentConverter.java:116)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.fromRow(RowToBsonDocumentConverter.java:105)\n\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:91)\n\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:44)\n\tat org.apache.spark.sql.connector.write.DataWriter.writeAll(DataWriter.java:108)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:587)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:483)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:535)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:466)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:584)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:427)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.lang.Thread.run(Thread.java:1583)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mSparkException\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mcreate_product_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m df\u001B[38;5;241m.\u001B[39mcolumns\n",
      "Cell \u001B[0;32mIn[5], line 27\u001B[0m, in \u001B[0;36mcreate_product_embeddings\u001B[0;34m(df, properties_column, embeddings_column)\u001B[0m\n\u001B[1;32m     20\u001B[0m source_column \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mproperties_column\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_flat\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     21\u001B[0m df \u001B[38;5;241m=\u001B[39m __compute_embeddings(df_flat, embeddings_column, source_column)\n\u001B[1;32m     22\u001B[0m (\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmongodb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark.mongodb.write.connection.uri\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMONGO_URL\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdatabase\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMONGO_DATABASE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcollection\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMONGO_PRODUCTS_COLLECTION\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mappend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m---> 27\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/UNI/TFM/code/mbd-tfm/api/src/.venv/lib/python3.10/site-packages/pyspark/sql/connect/readwriter.py:679\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    677\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m    678\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m=\u001B[39m path\n\u001B[0;32m--> 679\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute_command\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    680\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_write\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcommand\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_write\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobservations\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    682\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
      "File \u001B[0;32m~/UNI/TFM/code/mbd-tfm/api/src/.venv/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1148\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations)\u001B[0m\n\u001B[1;32m   1146\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1147\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1148\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execute_and_fetch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1149\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservations\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m   1150\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1151\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1152\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
      "File \u001B[0;32m~/UNI/TFM/code/mbd-tfm/api/src/.venv/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1560\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, self_destruct)\u001B[0m\n\u001B[1;32m   1557\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1560\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1561\u001B[0m         req, observations, progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1562\u001B[0m     ):\n\u001B[1;32m   1563\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1564\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
      "File \u001B[0;32m~/UNI/TFM/code/mbd-tfm/api/src/.venv/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1537\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, progress)\u001B[0m\n\u001B[1;32m   1535\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1537\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/UNI/TFM/code/mbd-tfm/api/src/.venv/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1811\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   1809\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1810\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 1811\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_rpc_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   1813\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[0;32m~/UNI/TFM/code/mbd-tfm/api/src/.venv/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1882\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   1879\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m info\u001B[38;5;241m.\u001B[39mmetadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merrorClass\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mINVALID_HANDLE.SESSION_CHANGED\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1880\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m-> 1882\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   1883\u001B[0m                 info,\n\u001B[1;32m   1884\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   1885\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   1886\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   1887\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1889\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mSparkException\u001B[0m: Writing job failed.\n\nJVM stacktrace:\norg.apache.spark.SparkException\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobFailedError(QueryExecutionErrors.scala:893)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:452)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:397)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:237)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:358)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:237)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:190)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:2954)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2492)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 7) (172.19.0.4 executor 6): com.mongodb.spark.sql.connector.exceptions.DataException: Cannot cast (128,[0,1,2,3,4,5,6,7,8,10,11,12,13,14,15,16,17,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,38,39,40,41,42,43,44,45,46,47,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,98,99,100,102,103,104,105,106,107,108,109,111,112,113,114,115,116,117,118,119,120,121,122,123,124,126,127],[3.0,2.0,5.0,2.0,7.0,5.0,1.0,1.0,5.0,2.0,3.0,1.0,3.0,1.0,2.0,1.0,8.0,4.0,1.0,1.0,2.0,3.0,1.0,2.0,3.0,1.0,4.0,4.0,6.0,2.0,3.0,1.0,1.0,3.0,2.0,2.0,4.0,2.0,5.0,5.0,3.0,5.0,2.0,1.0,3.0,3.0,1.0,1.0,5.0,2.0,2.0,4.0,3.0,3.0,5.0,4.0,3.0,5.0,3.0,4.0,3.0,3.0,3.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,4.0,2.0,3.0,6.0,2.0,4.0,3.0,2.0,1.0,3.0,2.0,3.0,8.0,3.0,2.0,2.0,3.0,1.0,5.0,1.0,4.0,3.0,5.0,3.0,2.0,2.0,2.0,3.0,2.0,3.0,5.0,5.0,2.0,1.0,5.0,1.0,2.0,1.0,3.0,2.0,5.0,5.0,2.0,1.0,1.0,2.0,3.0,6.0]) into a BsonValue. org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 has no matching BsonValue. Error: Cannot cast (128,[0,1,2,3,4,5,6,7,8,10,11,12,13,14,15,16,17,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,38,39,40,41,42,43,44,45,46,47,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,98,99,100,102,103,104,105,106,107,108,109,111,112,113,114,115,116,117,118,119,120,121,122,123,124,126,127],[3.0,2.0,5.0,2.0,7.0,5.0,1.0,1.0,5.0,2.0,3.0,1.0,3.0,1.0,2.0,1.0,8.0,4.0,1.0,1.0,2.0,3.0,1.0,2.0,3.0,1.0,4.0,4.0,6.0,2.0,3.0,1.0,1.0,3.0,2.0,2.0,4.0,2.0,5.0,5.0,3.0,5.0,2.0,1.0,3.0,3.0,1.0,1.0,5.0,2.0,2.0,4.0,3.0,3.0,5.0,4.0,3.0,5.0,3.0,4.0,3.0,3.0,3.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,4.0,2.0,3.0,6.0,2.0,4.0,3.0,2.0,1.0,3.0,2.0,3.0,8.0,3.0,2.0,2.0,3.0,1.0,5.0,1.0,4.0,3.0,5.0,3.0,2.0,2.0,2.0,3.0,2.0,3.0,5.0,5.0,2.0,1.0,5.0,1.0,2.0,1.0,3.0,2.0,5.0,5.0,2.0,1.0,1.0,2.0,3.0,6.0]) into a BsonValue. org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 data type has no matching BsonValue.\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.lambda$createObjectToBsonValue$be3154de$1(RowToBsonDocumentConverter.java:140)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.lambda$dataTypeToBsonElement$300467b5$1(RowToBsonDocumentConverter.java:153)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.rowToBsonDocument(RowToBsonDocumentConverter.java:270)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.fromRow(RowToBsonDocumentConverter.java:116)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.fromRow(RowToBsonDocumentConverter.java:105)\n\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:91)\n\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:44)\n\tat org.apache.spark.sql.connector.write.DataWriter.writeAll(DataWriter.java:108)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:587)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:483)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:535)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:466)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:584)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:427)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: com.mongodb.spark.sql.connector.exceptions.DataException: Write aborted for: PartitionId: 0, TaskId: 7. Manual data clean up may be required.\n\t\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.abort(MongoDataWriter.java:121)\n\t\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$10(WriteToDataSourceV2Exec.scala:528)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1334)\n\t\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:424)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:397)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:237)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:358)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:237)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:190)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:2954)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2492)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)\nCaused by: com.mongodb.spark.sql.connector.exceptions.DataException: Cannot cast (128,[0,1,2,3,4,5,6,7,8,10,11,12,13,14,15,16,17,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,38,39,40,41,42,43,44,45,46,47,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,98,99,100,102,103,104,105,106,107,108,109,111,112,113,114,115,116,117,118,119,120,121,122,123,124,126,127],[3.0,2.0,5.0,2.0,7.0,5.0,1.0,1.0,5.0,2.0,3.0,1.0,3.0,1.0,2.0,1.0,8.0,4.0,1.0,1.0,2.0,3.0,1.0,2.0,3.0,1.0,4.0,4.0,6.0,2.0,3.0,1.0,1.0,3.0,2.0,2.0,4.0,2.0,5.0,5.0,3.0,5.0,2.0,1.0,3.0,3.0,1.0,1.0,5.0,2.0,2.0,4.0,3.0,3.0,5.0,4.0,3.0,5.0,3.0,4.0,3.0,3.0,3.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,4.0,2.0,3.0,6.0,2.0,4.0,3.0,2.0,1.0,3.0,2.0,3.0,8.0,3.0,2.0,2.0,3.0,1.0,5.0,1.0,4.0,3.0,5.0,3.0,2.0,2.0,2.0,3.0,2.0,3.0,5.0,5.0,2.0,1.0,5.0,1.0,2.0,1.0,3.0,2.0,5.0,5.0,2.0,1.0,1.0,2.0,3.0,6.0]) into a BsonValue. org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 has no matching BsonValue. Error: Cannot cast (128,[0,1,2,3,4,5,6,7,8,10,11,12,13,14,15,16,17,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,38,39,40,41,42,43,44,45,46,47,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,98,99,100,102,103,104,105,106,107,108,109,111,112,113,114,115,116,117,118,119,120,121,122,123,124,126,127],[3.0,2.0,5.0,2.0,7.0,5.0,1.0,1.0,5.0,2.0,3.0,1.0,3.0,1.0,2.0,1.0,8.0,4.0,1.0,1.0,2.0,3.0,1.0,2.0,3.0,1.0,4.0,4.0,6.0,2.0,3.0,1.0,1.0,3.0,2.0,2.0,4.0,2.0,5.0,5.0,3.0,5.0,2.0,1.0,3.0,3.0,1.0,1.0,5.0,2.0,2.0,4.0,3.0,3.0,5.0,4.0,3.0,5.0,3.0,4.0,3.0,3.0,3.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,4.0,2.0,3.0,6.0,2.0,4.0,3.0,2.0,1.0,3.0,2.0,3.0,8.0,3.0,2.0,2.0,3.0,1.0,5.0,1.0,4.0,3.0,5.0,3.0,2.0,2.0,2.0,3.0,2.0,3.0,5.0,5.0,2.0,1.0,5.0,1.0,2.0,1.0,3.0,2.0,5.0,5.0,2.0,1.0,1.0,2.0,3.0,6.0]) into a BsonValue. org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 data type has no matching BsonValue.\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.lambda$createObjectToBsonValue$be3154de$1(RowToBsonDocumentConverter.java:140)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.lambda$dataTypeToBsonElement$300467b5$1(RowToBsonDocumentConverter.java:153)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.rowToBsonDocument(RowToBsonDocumentConverter.java:270)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.fromRow(RowToBsonDocumentConverter.java:116)\n\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.fromRow(RowToBsonDocumentConverter.java:105)\n\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:91)\n\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.write(MongoDataWriter.java:44)\n\tat org.apache.spark.sql.connector.write.DataWriter.writeAll(DataWriter.java:108)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:587)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:483)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:535)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:466)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:584)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:427)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.lang.Thread.run(Thread.java:1583)"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
